{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 2.5244e-29, 0.0000e+00],\n",
       "        [2.5244e-29, 4.5036e+16, 4.5891e-41],\n",
       "        [4.5212e+16, 4.5891e-41, 9.1624e+16],\n",
       "        [4.5891e-41, 9.1625e+16, 4.5891e-41],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.empty(5,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1551, 0.5197, 0.8950],\n",
       "        [0.6641, 0.5146, 0.3308],\n",
       "        [0.7671, 0.2130, 0.7745]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([5,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.new_ones(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3457,  1.4979,  0.5092],\n",
       "        [-0.8770,  0.4985, -1.1250],\n",
       "        [-1.9603, -0.4960,  1.7117],\n",
       "        [ 0.0112,  0.2112,  0.0943],\n",
       "        [-0.4114, -0.6250,  0.0588]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.randn_like(a, dtype=torch.float)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.arange(9).reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.view(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = c.view(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31782490.84776744\n",
      "1 32874316.50947191\n",
      "2 39480200.41511234\n",
      "3 43808063.06674564\n",
      "4 38394125.09032567\n",
      "5 23980616.91481995\n",
      "6 11228600.289973363\n",
      "7 4749504.671887663\n",
      "8 2359442.3163579246\n",
      "9 1500457.229420456\n",
      "10 1133845.0474635994\n",
      "11 928537.5552468299\n",
      "12 786280.1189433441\n",
      "13 675936.479805172\n",
      "14 586053.9492437374\n",
      "15 511154.30651079153\n",
      "16 448001.5731074206\n",
      "17 394475.2807865023\n",
      "18 348690.8242354236\n",
      "19 309306.69705367635\n",
      "20 275230.2276803929\n",
      "21 245630.95654942637\n",
      "22 219804.99111222784\n",
      "23 197182.65883805972\n",
      "24 177311.93153614787\n",
      "25 159846.24066554112\n",
      "26 144405.1898131962\n",
      "27 130700.91980516454\n",
      "28 118510.28392792324\n",
      "29 107648.96309927295\n",
      "30 97948.41897366129\n",
      "31 89261.6889634885\n",
      "32 81478.27797832908\n",
      "33 74489.62966864911\n",
      "34 68198.2873260086\n",
      "35 62519.60821467811\n",
      "36 57396.38685221712\n",
      "37 52760.7700630193\n",
      "38 48555.85080232244\n",
      "39 44737.91997699311\n",
      "40 41265.53419533675\n",
      "41 38116.228641370384\n",
      "42 35252.92112814999\n",
      "43 32640.79343817533\n",
      "44 30253.59050826759\n",
      "45 28070.619694452285\n",
      "46 26069.253993839477\n",
      "47 24232.78190097434\n",
      "48 22546.024008820445\n",
      "49 20994.54706295386\n",
      "50 19567.544913916645\n",
      "51 18252.86161321634\n",
      "52 17041.71679292748\n",
      "53 15922.952944068717\n",
      "54 14889.686005121112\n",
      "55 13933.811793146964\n",
      "56 13049.07919056754\n",
      "57 12228.890294617522\n",
      "58 11468.469384560425\n",
      "59 10762.8760195055\n",
      "60 10107.411432980643\n",
      "61 9497.909393246358\n",
      "62 8930.551640389009\n",
      "63 8402.306876954122\n",
      "64 7910.461480763851\n",
      "65 7451.763204397463\n",
      "66 7023.31209261835\n",
      "67 6623.393249318865\n",
      "68 6249.297470357285\n",
      "69 5899.341705249084\n",
      "70 5571.805510356371\n",
      "71 5265.511870810657\n",
      "72 4978.535413051516\n",
      "73 4709.2464533811035\n",
      "74 4456.591365895958\n",
      "75 4219.29508085191\n",
      "76 3996.51161647975\n",
      "77 3787.1029004425013\n",
      "78 3590.1086083538985\n",
      "79 3405.0325801334902\n",
      "80 3230.548672896998\n",
      "81 3066.219696448098\n",
      "82 2911.349754048244\n",
      "83 2765.234669392134\n",
      "84 2627.0966213636602\n",
      "85 2496.689366955224\n",
      "86 2373.6614545676853\n",
      "87 2257.482207261698\n",
      "88 2147.602555034776\n",
      "89 2043.7968551753943\n",
      "90 1945.5190543051565\n",
      "91 1852.6008675698858\n",
      "92 1764.5565242799794\n",
      "93 1681.2065100230577\n",
      "94 1602.2638646220898\n",
      "95 1527.4087140199572\n",
      "96 1457.4683468066655\n",
      "97 1391.3693760504857\n",
      "98 1328.666245956799\n",
      "99 1269.0922567743648\n",
      "100 1212.5427569538526\n",
      "101 1158.787156184746\n",
      "102 1107.677332618819\n",
      "103 1059.0706542945743\n",
      "104 1012.8452035275897\n",
      "105 968.8715297713383\n",
      "106 927.0273541835486\n",
      "107 887.1853223865062\n",
      "108 849.23059081144\n",
      "109 813.1017010538757\n",
      "110 778.6258339443534\n",
      "111 745.741874008017\n",
      "112 714.3832859006889\n",
      "113 684.4859067924677\n",
      "114 655.9539966919474\n",
      "115 628.7183425924836\n",
      "116 602.7122835574283\n",
      "117 577.88755819116\n",
      "118 554.1819112658933\n",
      "119 531.5364729497018\n",
      "120 509.88926885841465\n",
      "121 489.2069687016447\n",
      "122 469.44079257544394\n",
      "123 450.5383754711501\n",
      "124 432.46388696928676\n",
      "125 415.1773990682777\n",
      "126 398.65260472689704\n",
      "127 382.80903852774685\n",
      "128 367.67972991346414\n",
      "129 353.1636138201786\n",
      "130 339.2452382508682\n",
      "131 325.9155574842005\n",
      "132 313.1489189879983\n",
      "133 300.9148610293961\n",
      "134 289.1977883293865\n",
      "135 277.97170420098104\n",
      "136 267.21182760207665\n",
      "137 256.8914464579232\n",
      "138 247.00147302621326\n",
      "139 237.517439120755\n",
      "140 228.41955019690215\n",
      "141 219.6956818683709\n",
      "142 211.32534373713617\n",
      "143 203.29278906915363\n",
      "144 195.5958876650156\n",
      "145 188.2058110887902\n",
      "146 181.10516739320087\n",
      "147 174.28743860003453\n",
      "148 167.7430943512512\n",
      "149 161.45685683053756\n",
      "150 155.4215914741086\n",
      "151 149.63105832618038\n",
      "152 144.06017106530854\n",
      "153 138.710074686825\n",
      "154 133.56920928554112\n",
      "155 128.62984682998686\n",
      "156 123.87984456761814\n",
      "157 119.31338356235358\n",
      "158 114.92477436878607\n",
      "159 110.704811873003\n",
      "160 106.64671092323786\n",
      "161 102.7464660619205\n",
      "162 98.99699082777757\n",
      "163 95.3912244563599\n",
      "164 91.92371446472097\n",
      "165 88.58372436052395\n",
      "166 85.37057055128872\n",
      "167 82.27976308511388\n",
      "168 79.30555155955378\n",
      "169 76.4433091993282\n",
      "170 73.68923620402255\n",
      "171 71.03823068177111\n",
      "172 68.48596483242656\n",
      "173 66.02913988325311\n",
      "174 63.66401644676483\n",
      "175 61.38695856295204\n",
      "176 59.19522341354299\n",
      "177 57.08449758917774\n",
      "178 55.05412272331454\n",
      "179 53.09693527980291\n",
      "180 51.2106733218579\n",
      "181 49.39457664315586\n",
      "182 47.64499923224974\n",
      "183 45.96055798622833\n",
      "184 44.336812685371775\n",
      "185 42.77195798800826\n",
      "186 41.26394743888014\n",
      "187 39.811542606111956\n",
      "188 38.41242719490288\n",
      "189 37.063087560871566\n",
      "190 35.762538706924346\n",
      "191 34.50931944422226\n",
      "192 33.30092521098064\n",
      "193 32.136214745573\n",
      "194 31.013317674263384\n",
      "195 29.930954147317635\n",
      "196 28.887696542926868\n",
      "197 27.88159971348165\n",
      "198 26.911492076311802\n",
      "199 25.976170826380056\n",
      "200 25.074507470148028\n",
      "201 24.20448271640528\n",
      "202 23.365536798138606\n",
      "203 22.556843440485803\n",
      "204 21.77697476989208\n",
      "205 21.02409903905299\n",
      "206 20.29844950965243\n",
      "207 19.59796358443933\n",
      "208 18.922857194949216\n",
      "209 18.271690256509746\n",
      "210 17.642769815325494\n",
      "211 17.036028174778238\n",
      "212 16.450579669079072\n",
      "213 15.885767690112177\n",
      "214 15.340699646188222\n",
      "215 14.814784421945602\n",
      "216 14.307446569851537\n",
      "217 13.817930172293329\n",
      "218 13.345381215731104\n",
      "219 12.889450179726989\n",
      "220 12.44917697654898\n",
      "221 12.024320680738406\n",
      "222 11.614589969105383\n",
      "223 11.218759570125696\n",
      "224 10.836994925418104\n",
      "225 10.468076000314863\n",
      "226 10.112053294529415\n",
      "227 9.768303615904083\n",
      "228 9.436578778066002\n",
      "229 9.116158684059132\n",
      "230 8.806879547548533\n",
      "231 8.508295414621653\n",
      "232 8.219976829602503\n",
      "233 7.941604694789014\n",
      "234 7.672803005362352\n",
      "235 7.413279133357218\n",
      "236 7.1626510528346286\n",
      "237 6.920742733822191\n",
      "238 6.686990348623638\n",
      "239 6.461295799339924\n",
      "240 6.243360067044511\n",
      "241 6.033004088597932\n",
      "242 5.8299322415513\n",
      "243 5.633551282524568\n",
      "244 5.4439220245772715\n",
      "245 5.260925414806026\n",
      "246 5.084002354226643\n",
      "247 4.9131249027625055\n",
      "248 4.748026992242987\n",
      "249 4.588597299599024\n",
      "250 4.434571776194064\n",
      "251 4.2857860426660075\n",
      "252 4.142071381670862\n",
      "253 4.003263669437869\n",
      "254 3.8691473048495064\n",
      "255 3.739617757092395\n",
      "256 3.6144578805938377\n",
      "257 3.493635343466163\n",
      "258 3.3767801711932637\n",
      "259 3.2639079119371575\n",
      "260 3.1548312811286197\n",
      "261 3.0494751283348798\n",
      "262 2.947633446415238\n",
      "263 2.8492432488675323\n",
      "264 2.7541775753891438\n",
      "265 2.6623172856331996\n",
      "266 2.5736115406839186\n",
      "267 2.4878337422453636\n",
      "268 2.4049688396392623\n",
      "269 2.324875618782518\n",
      "270 2.247484414549845\n",
      "271 2.172688554318952\n",
      "272 2.100412114423475\n",
      "273 2.030573427294928\n",
      "274 1.9630931281308044\n",
      "275 1.8978533809328941\n",
      "276 1.8348702724819295\n",
      "277 1.7739759319353874\n",
      "278 1.715080427648191\n",
      "279 1.658164597452799\n",
      "280 1.6031439851022404\n",
      "281 1.5499737901597417\n",
      "282 1.4985728407711025\n",
      "283 1.4489047162569615\n",
      "284 1.4008946245328224\n",
      "285 1.35448036984591\n",
      "286 1.3096316996816222\n",
      "287 1.2662892810877868\n",
      "288 1.2243966184448734\n",
      "289 1.1838846915755545\n",
      "290 1.1447171265813851\n",
      "291 1.1068652082123687\n",
      "292 1.0702796337883198\n",
      "293 1.034947521305165\n",
      "294 1.000749344954672\n",
      "295 0.967686624531719\n",
      "296 0.9357299888561283\n",
      "297 0.9048357333779113\n",
      "298 0.874962962653883\n",
      "299 0.8460900870585905\n",
      "300 0.8181777838457838\n",
      "301 0.7911949120989262\n",
      "302 0.7651051472319699\n",
      "303 0.7398777249964372\n",
      "304 0.7154926281408135\n",
      "305 0.6919151604160027\n",
      "306 0.6691166122501722\n",
      "307 0.6470795368956761\n",
      "308 0.6257738681356624\n",
      "309 0.6051767646477331\n",
      "310 0.5852573349305836\n",
      "311 0.5659958458344989\n",
      "312 0.5474066777690895\n",
      "313 0.5294042284222948\n",
      "314 0.5119923886906094\n",
      "315 0.49516149021825984\n",
      "316 0.4788846045376464\n",
      "317 0.46314209130940964\n",
      "318 0.4479251206236932\n",
      "319 0.4332075644341148\n",
      "320 0.41897876338690365\n",
      "321 0.4052199399216558\n",
      "322 0.3919127019943663\n",
      "323 0.3790466235826996\n",
      "324 0.3666067095895049\n",
      "325 0.3545745568709553\n",
      "326 0.3429405129884913\n",
      "327 0.33169025288341203\n",
      "328 0.3208101047640677\n",
      "329 0.31030347872580993\n",
      "330 0.30013454693902253\n",
      "331 0.29029519013508565\n",
      "332 0.2807850555476948\n",
      "333 0.27158260355558567\n",
      "334 0.2626828403631774\n",
      "335 0.25407853821309\n",
      "336 0.2457551740028739\n",
      "337 0.23770695872458195\n",
      "338 0.22992344983497895\n",
      "339 0.22239600997878\n",
      "340 0.21511592244255368\n",
      "341 0.20807710496095172\n",
      "342 0.20126681053918202\n",
      "343 0.1946821188911485\n",
      "344 0.18831390683954305\n",
      "345 0.18215421097294626\n",
      "346 0.17619666490192737\n",
      "347 0.17043476467885132\n",
      "348 0.16487052416896344\n",
      "349 0.15948187148263884\n",
      "350 0.15426934033353307\n",
      "351 0.1492290973349275\n",
      "352 0.1443539284718057\n",
      "353 0.13963756256008245\n",
      "354 0.13507520606489754\n",
      "355 0.1306631369818154\n",
      "356 0.12639529332081756\n",
      "357 0.12226750390198146\n",
      "358 0.11827625081846214\n",
      "359 0.1144149587193553\n",
      "360 0.11067949861175037\n",
      "361 0.1070676556800734\n",
      "362 0.10357316725972228\n",
      "363 0.10019321013055936\n",
      "364 0.0969242712966757\n",
      "365 0.09376498237458054\n",
      "366 0.09070678117148294\n",
      "367 0.08774848726500402\n",
      "368 0.08488685673804128\n",
      "369 0.08211848681566342\n",
      "370 0.07944108052409518\n",
      "371 0.07685187374203438\n",
      "372 0.07434737261270392\n",
      "373 0.07192457259395066\n",
      "374 0.06958028179344172\n",
      "375 0.0673127338395802\n",
      "376 0.06511938692796534\n",
      "377 0.06299789372216369\n",
      "378 0.060945217414838014\n",
      "379 0.058960029478138366\n",
      "380 0.05703950860029754\n",
      "381 0.05518160367039472\n",
      "382 0.05338469279710476\n",
      "383 0.05164641133432384\n",
      "384 0.04996729348937075\n",
      "385 0.04834061450524221\n",
      "386 0.04676758574871213\n",
      "387 0.045245184541654423\n",
      "388 0.04377254956130039\n",
      "389 0.04234802247682137\n",
      "390 0.04096975529371148\n",
      "391 0.03963684296643926\n",
      "392 0.03834745659427621\n",
      "393 0.03710016579902666\n",
      "394 0.03589315839959556\n",
      "395 0.03472585827008857\n",
      "396 0.033596268709788636\n",
      "397 0.03250361190029381\n",
      "398 0.03144675315025676\n",
      "399 0.03042407651895293\n",
      "400 0.029434887165018884\n",
      "401 0.028478691516876646\n",
      "402 0.02755342718293108\n",
      "403 0.026657626719506364\n",
      "404 0.025791307971593916\n",
      "405 0.02495305637058954\n",
      "406 0.024142006653214584\n",
      "407 0.023357470728303246\n",
      "408 0.02259848062152583\n",
      "409 0.02186414834693268\n",
      "410 0.021153746132374515\n",
      "411 0.020466557968662407\n",
      "412 0.019801791036209684\n",
      "413 0.0191587155405328\n",
      "414 0.018536628868798404\n",
      "415 0.01793463665669412\n",
      "416 0.017352131995756144\n",
      "417 0.0167886702908967\n",
      "418 0.016243457624807448\n",
      "419 0.01571598855883236\n",
      "420 0.015206158819972726\n",
      "421 0.014712851404680826\n",
      "422 0.014235233681096639\n",
      "423 0.013773113130920497\n",
      "424 0.013326079806469198\n",
      "425 0.012893481735224591\n",
      "426 0.012474965750411769\n",
      "427 0.012070145416688254\n",
      "428 0.011678425350258252\n",
      "429 0.011299488402258445\n",
      "430 0.010932852094939067\n",
      "431 0.010578139918907692\n",
      "432 0.010234933952609188\n",
      "433 0.009903009979089012\n",
      "434 0.009581785967272754\n",
      "435 0.009270935574022102\n",
      "436 0.008970185723392357\n",
      "437 0.00867929726235256\n",
      "438 0.0083980750014047\n",
      "439 0.008125685069624519\n",
      "440 0.007862210521614572\n",
      "441 0.007607249364295981\n",
      "442 0.00736055441142381\n",
      "443 0.0071219202540849826\n",
      "444 0.00689104440171025\n",
      "445 0.006667591290692336\n",
      "446 0.006451412791141\n",
      "447 0.006242278635908798\n",
      "448 0.006039897392228004\n",
      "449 0.005844108854874883\n",
      "450 0.005654688425287448\n",
      "451 0.0054713994194201705\n",
      "452 0.005294039286694329\n",
      "453 0.005122487527637361\n",
      "454 0.004956529176993852\n",
      "455 0.004795884093244964\n",
      "456 0.004640557537266719\n",
      "457 0.004490324812407886\n",
      "458 0.004344849130977201\n",
      "459 0.004204062450701981\n",
      "460 0.004067854937674095\n",
      "461 0.003936064072564251\n",
      "462 0.0038085384714558954\n",
      "463 0.0036851504207030656\n",
      "464 0.003565774406491564\n",
      "465 0.003450256106847493\n",
      "466 0.003338486248304115\n",
      "467 0.0032303574400762464\n",
      "468 0.0031257276347692493\n",
      "469 0.0030244840434931804\n",
      "470 0.0029265308420938566\n",
      "471 0.0028317498761715964\n",
      "472 0.002740056776579178\n",
      "473 0.002651332621722535\n",
      "474 0.002565550141628916\n",
      "475 0.002482520689917401\n",
      "476 0.0024021324659893326\n",
      "477 0.0023243670985635856\n",
      "478 0.0022491167759607885\n",
      "479 0.002176293573811781\n",
      "480 0.0021058290283607114\n",
      "481 0.0020376627542159453\n",
      "482 0.001971690865077899\n",
      "483 0.001907858025723704\n",
      "484 0.0018461007998725632\n",
      "485 0.0017863399499143497\n",
      "486 0.0017285216228195683\n",
      "487 0.0016725761975336974\n",
      "488 0.0016184455505132503\n",
      "489 0.0015660580006839704\n",
      "490 0.0015153660976974905\n",
      "491 0.0014663230911570967\n",
      "492 0.001418918872701257\n",
      "493 0.0013730223817050306\n",
      "494 0.001328585572177867\n",
      "495 0.0012855978422264356\n",
      "496 0.0012439976272888209\n",
      "497 0.0012037466961477184\n",
      "498 0.0011647972730251242\n",
      "499 0.0011271100781655157\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "l = 1e-6\n",
    "\n",
    "for it in range(500):\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(it, loss)\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    w1 -= l * grad_w1\n",
    "    w2 -= l * grad_w2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34029098.20785901\n",
      "1 28327529.286202136\n",
      "2 25198985.561848875\n",
      "3 21165898.8660159\n",
      "4 15982424.265181433\n",
      "5 10789220.8027098\n",
      "6 6784685.216454179\n",
      "7 4184933.378474255\n",
      "8 2661311.73332985\n",
      "9 1795142.302863257\n",
      "10 1294100.250338194\n",
      "11 988998.9277855998\n",
      "12 790014.0260108977\n",
      "13 650854.7012714611\n",
      "14 547446.256055794\n",
      "15 467010.01122655044\n",
      "16 402342.5345227994\n",
      "17 349179.50459482876\n",
      "18 304797.180086375\n",
      "19 267423.56764794595\n",
      "20 235579.41585731736\n",
      "21 208258.92971410073\n",
      "22 184690.4172529775\n",
      "23 164258.58358242176\n",
      "24 146461.69401297832\n",
      "25 130902.327120658\n",
      "26 117251.06775162296\n",
      "27 105241.53173915998\n",
      "28 94654.42747544406\n",
      "29 85286.86075705125\n",
      "30 76976.77680123654\n",
      "31 69585.54626139863\n",
      "32 62998.935977146044\n",
      "33 57119.080584079784\n",
      "34 51855.22186773147\n",
      "35 47137.77052496301\n",
      "36 42901.76581168402\n",
      "37 39092.71739696649\n",
      "38 35676.674318242\n",
      "39 32596.184163571557\n",
      "40 29812.279889410132\n",
      "41 27293.224000837195\n",
      "42 25010.188884742827\n",
      "43 22937.92881692147\n",
      "44 21054.700202004657\n",
      "45 19342.02188515587\n",
      "46 17782.976580689046\n",
      "47 16363.153984014629\n",
      "48 15068.608752995397\n",
      "49 13886.455133239724\n",
      "50 12805.769419767948\n",
      "51 11817.250845673841\n",
      "52 10912.068225652694\n",
      "53 10083.208671674114\n",
      "54 9323.553350856695\n",
      "55 8626.277573692616\n",
      "56 7985.977494309546\n",
      "57 7396.986586189583\n",
      "58 6855.2345637474355\n",
      "59 6356.45195515323\n",
      "60 5896.854976462623\n",
      "61 5473.334521664382\n",
      "62 5082.514551151729\n",
      "63 4721.613726938545\n",
      "64 4388.300911565076\n",
      "65 4080.4507877901806\n",
      "66 3795.849969653131\n",
      "67 3532.6149864649055\n",
      "68 3289.0288060558246\n",
      "69 3063.430031474213\n",
      "70 2854.629237097248\n",
      "71 2661.069417624045\n",
      "72 2481.5341954631363\n",
      "73 2314.933250760625\n",
      "74 2160.2810958092914\n",
      "75 2016.7082434806437\n",
      "76 1883.2861235264784\n",
      "77 1759.3326516654631\n",
      "78 1644.0255820559\n",
      "79 1536.82618202195\n",
      "80 1436.9682669088502\n",
      "81 1343.9968740242089\n",
      "82 1257.4335222128861\n",
      "83 1176.8256221713818\n",
      "84 1101.6805363355816\n",
      "85 1031.603534485617\n",
      "86 966.219259593838\n",
      "87 905.2379243854884\n",
      "88 848.358960736438\n",
      "89 795.2043624061281\n",
      "90 745.5676249491986\n",
      "91 699.217108453996\n",
      "92 655.9210852775452\n",
      "93 615.4228070863761\n",
      "94 577.5520190513653\n",
      "95 542.1432749888908\n",
      "96 509.0297912380973\n",
      "97 478.0586999491032\n",
      "98 449.03593997126677\n",
      "99 421.87061803383955\n",
      "100 396.43447278249835\n",
      "101 372.6151054384473\n",
      "102 350.29594905825275\n",
      "103 329.38908746248217\n",
      "104 309.7851493064758\n",
      "105 291.4003813782975\n",
      "106 274.1802665136868\n",
      "107 258.02715494140205\n",
      "108 242.8766281358379\n",
      "109 228.65256992272163\n",
      "110 215.3070604286118\n",
      "111 202.77083701234503\n",
      "112 190.99151131757566\n",
      "113 179.92788449486875\n",
      "114 169.53550923978278\n",
      "115 159.76951953852193\n",
      "116 150.58915326618362\n",
      "117 141.95833661832606\n",
      "118 133.84654060434752\n",
      "119 126.21296882241467\n",
      "120 119.03380064300418\n",
      "121 112.27719314204566\n",
      "122 105.92166891517508\n",
      "123 99.93961645333695\n",
      "124 94.30975271658393\n",
      "125 89.00795265207907\n",
      "126 84.01466167995744\n",
      "127 79.31471693694402\n",
      "128 74.88729009863377\n",
      "129 70.71456366289428\n",
      "130 66.78404263999361\n",
      "131 63.08148497450136\n",
      "132 59.591803872549704\n",
      "133 56.300760247943614\n",
      "134 53.19761707850421\n",
      "135 50.272690016765885\n",
      "136 47.51473023094316\n",
      "137 44.91336669307597\n",
      "138 42.45868994737966\n",
      "139 40.14354296314216\n",
      "140 37.95899352121577\n",
      "141 35.897485246093495\n",
      "142 33.951531400481144\n",
      "143 32.11461158329509\n",
      "144 30.38067632749363\n",
      "145 28.744176102998548\n",
      "146 27.197790330506834\n",
      "147 25.737538783831717\n",
      "148 24.35829910331268\n",
      "149 23.055840386379725\n",
      "150 21.825688644544513\n",
      "151 20.66269112212563\n",
      "152 19.563663468988395\n",
      "153 18.525082015538146\n",
      "154 17.543399303031087\n",
      "155 16.61520837676752\n",
      "156 15.73802875652339\n",
      "157 14.90851772215227\n",
      "158 14.124384778507661\n",
      "159 13.382155799284968\n",
      "160 12.680114353990078\n",
      "161 12.016490243471846\n",
      "162 11.38873914718062\n",
      "163 10.794523224884486\n",
      "164 10.232091852610111\n",
      "165 9.699844161677131\n",
      "166 9.196488159500227\n",
      "167 8.71998519672329\n",
      "168 8.268707725050339\n",
      "169 7.8413766341133435\n",
      "170 7.436881874572072\n",
      "171 7.053962313377053\n",
      "172 6.6911955567594\n",
      "173 6.347932671720809\n",
      "174 6.023627571974107\n",
      "175 5.716556758505838\n",
      "176 5.425400636012405\n",
      "177 5.149483377255068\n",
      "178 4.888039540706747\n",
      "179 4.640355609747261\n",
      "180 4.4056211894365065\n",
      "181 4.182976952037992\n",
      "182 3.9718916492417176\n",
      "183 3.7718538109824213\n",
      "184 3.5822349994025\n",
      "185 3.4023627051985263\n",
      "186 3.231739885813055\n",
      "187 3.069927784865735\n",
      "188 2.916449260303967\n",
      "189 2.770843985516653\n",
      "190 2.632682005985605\n",
      "191 2.501677743497074\n",
      "192 2.377334533980125\n",
      "193 2.2593827135278577\n",
      "194 2.147387673992285\n",
      "195 2.0410968201098285\n",
      "196 1.9402470712367605\n",
      "197 1.8444883094496354\n",
      "198 1.753583232124466\n",
      "199 1.6672641517351128\n",
      "200 1.5853336592756506\n",
      "201 1.5075091437406583\n",
      "202 1.4336479025141649\n",
      "203 1.3634657808704815\n",
      "204 1.2968249305776736\n",
      "205 1.2335126149927487\n",
      "206 1.1733722362807917\n",
      "207 1.1162668798046977\n",
      "208 1.0619904383369847\n",
      "209 1.010404857047952\n",
      "210 0.9613846295749193\n",
      "211 0.9148354215080396\n",
      "212 0.8705951079503241\n",
      "213 0.8285213638545211\n",
      "214 0.7885455258072365\n",
      "215 0.7505489931134071\n",
      "216 0.7144306977689072\n",
      "217 0.6801046454297618\n",
      "218 0.6474445600541123\n",
      "219 0.6163980504289979\n",
      "220 0.5868841176251389\n",
      "221 0.5588030212452977\n",
      "222 0.5321134075566099\n",
      "223 0.506727622961157\n",
      "224 0.48257261555177566\n",
      "225 0.45961024721979915\n",
      "226 0.4377563421357886\n",
      "227 0.4169733961574088\n",
      "228 0.3971934833708688\n",
      "229 0.3783726449700794\n",
      "230 0.3604667526975906\n",
      "231 0.3434275267965791\n",
      "232 0.3272149895153734\n",
      "233 0.3117807195476793\n",
      "234 0.29710007491688367\n",
      "235 0.2831187209018589\n",
      "236 0.2698099800995155\n",
      "237 0.2571488988791142\n",
      "238 0.24508958903953654\n",
      "239 0.2336113802030677\n",
      "240 0.22267752895755463\n",
      "241 0.21227166372973091\n",
      "242 0.20236377857847015\n",
      "243 0.19292473993551434\n",
      "244 0.18393744276129836\n",
      "245 0.17537952198157084\n",
      "246 0.1672279660207597\n",
      "247 0.15946316295191265\n",
      "248 0.15206635387296596\n",
      "249 0.14502210991439843\n",
      "250 0.1383082212392705\n",
      "251 0.13191348960578342\n",
      "252 0.12582087202097086\n",
      "253 0.12001407195055143\n",
      "254 0.11448149171111541\n",
      "255 0.10920948874598098\n",
      "256 0.10418691854912646\n",
      "257 0.09939790951346761\n",
      "258 0.09483420855564009\n",
      "259 0.09048586823122978\n",
      "260 0.08634006957638198\n",
      "261 0.08238836370494455\n",
      "262 0.0786209548838283\n",
      "263 0.07502912684219681\n",
      "264 0.07160581198618698\n",
      "265 0.06834200284632562\n",
      "266 0.06522878352083762\n",
      "267 0.06225991705060256\n",
      "268 0.059428665977086445\n",
      "269 0.05673113853173202\n",
      "270 0.0541568849116426\n",
      "271 0.051700884533401915\n",
      "272 0.04935928166065873\n",
      "273 0.0471257188861246\n",
      "274 0.04499600755760967\n",
      "275 0.04296362941061223\n",
      "276 0.041024946043951134\n",
      "277 0.03917543003639329\n",
      "278 0.037411284440652254\n",
      "279 0.035728275253135815\n",
      "280 0.0341215397283708\n",
      "281 0.03258835116819593\n",
      "282 0.031126302234979288\n",
      "283 0.029730572314461155\n",
      "284 0.02839838065159394\n",
      "285 0.027127363929699867\n",
      "286 0.025914117092310868\n",
      "287 0.024756155564101146\n",
      "288 0.023650771283919618\n",
      "289 0.02259570230536425\n",
      "290 0.021588595067550218\n",
      "291 0.020627345697816518\n",
      "292 0.019709680199833064\n",
      "293 0.018833195956298475\n",
      "294 0.01799649316610094\n",
      "295 0.017197987634692654\n",
      "296 0.016435338351503705\n",
      "297 0.01570696243350448\n",
      "298 0.0150116060808998\n",
      "299 0.0143475040754315\n",
      "300 0.013713302438751478\n",
      "301 0.013107735181523757\n",
      "302 0.01252912201438721\n",
      "303 0.011976495578185583\n",
      "304 0.011448939476824252\n",
      "305 0.010944776995001817\n",
      "306 0.01046326850494472\n",
      "307 0.010003350252104616\n",
      "308 0.009563993082804543\n",
      "309 0.009144318586337873\n",
      "310 0.008743241399481884\n",
      "311 0.008359972416003333\n",
      "312 0.007993928751988934\n",
      "313 0.007644158201188525\n",
      "314 0.0073098320476890025\n",
      "315 0.006990383517115723\n",
      "316 0.006685225469710064\n",
      "317 0.00639352500090639\n",
      "318 0.006114734723188514\n",
      "319 0.005848351868882432\n",
      "320 0.005593771497756863\n",
      "321 0.005350379212203172\n",
      "322 0.005117780675887779\n",
      "323 0.004895400691845754\n",
      "324 0.004682901972742563\n",
      "325 0.004479810689472482\n",
      "326 0.00428555649337058\n",
      "327 0.004099851111935459\n",
      "328 0.003922410738902487\n",
      "329 0.0037527086747196816\n",
      "330 0.003590469645874712\n",
      "331 0.0034353825727064437\n",
      "332 0.0032870551818732644\n",
      "333 0.00314522956594775\n",
      "334 0.0030096313442307923\n",
      "335 0.0028799332042560807\n",
      "336 0.0027559406929646415\n",
      "337 0.0026373889068337843\n",
      "338 0.0025239365971626406\n",
      "339 0.0024154559822014195\n",
      "340 0.0023117318352826226\n",
      "341 0.002212502186515739\n",
      "342 0.002117604457831066\n",
      "343 0.002026837778702279\n",
      "344 0.0019400105145089734\n",
      "345 0.0018569801027252043\n",
      "346 0.0017775242919017059\n",
      "347 0.0017015044502861247\n",
      "348 0.0016288101948921958\n",
      "349 0.0015592530036821774\n",
      "350 0.0014927086873341973\n",
      "351 0.0014290381748374742\n",
      "352 0.0013681260622791505\n",
      "353 0.0013098430319521406\n",
      "354 0.0012540714888350415\n",
      "355 0.0012006952304320809\n",
      "356 0.0011496573755250161\n",
      "357 0.0011007897054550568\n",
      "358 0.0010540230903925748\n",
      "359 0.0010092831573044869\n",
      "360 0.0009664725910753589\n",
      "361 0.0009254910196871091\n",
      "362 0.0008862726127462832\n",
      "363 0.0008487367152358788\n",
      "364 0.0008128231204702891\n",
      "365 0.0007784302597821052\n",
      "366 0.000745508422623882\n",
      "367 0.0007140078113152613\n",
      "368 0.000683852706488584\n",
      "369 0.0006549793397960162\n",
      "370 0.0006273461477641386\n",
      "371 0.0006008967475374168\n",
      "372 0.0005755739060493061\n",
      "373 0.0005513285829067977\n",
      "374 0.0005281150143299602\n",
      "375 0.0005059027026415406\n",
      "376 0.0004846235841184579\n",
      "377 0.0004642528431285563\n",
      "378 0.0004447527118518279\n",
      "379 0.0004260800513607835\n",
      "380 0.00040819870390847656\n",
      "381 0.0003910735854667168\n",
      "382 0.000374677818743812\n",
      "383 0.000358980611270844\n",
      "384 0.00034394317880909674\n",
      "385 0.000329546553503682\n",
      "386 0.0003157645263623134\n",
      "387 0.000302560667905407\n",
      "388 0.0002899131804312155\n",
      "389 0.000277801029975358\n",
      "390 0.0002662046910231549\n",
      "391 0.0002550930359233373\n",
      "392 0.0002444495356054031\n",
      "393 0.0002342584413205419\n",
      "394 0.00022449717890903937\n",
      "395 0.00021514456957067547\n",
      "396 0.0002061858392164848\n",
      "397 0.0001976068089517964\n",
      "398 0.00018938790515778275\n",
      "399 0.00018151266072595956\n",
      "400 0.00017396873933694284\n",
      "401 0.00016674436083524065\n",
      "402 0.00015982081567025765\n",
      "403 0.0001531878376375301\n",
      "404 0.00014683308920390026\n",
      "405 0.00014074584113740266\n",
      "406 0.00013491164213490743\n",
      "407 0.00012932253339328475\n",
      "408 0.0001239691453856488\n",
      "409 0.0001188374938007125\n",
      "410 0.00011392151477228716\n",
      "411 0.00010921205317079682\n",
      "412 0.00010469830641752982\n",
      "413 0.00010037199653252843\n",
      "414 9.622619265553319e-05\n",
      "415 9.225514816500562e-05\n",
      "416 8.844807450272648e-05\n",
      "417 8.47994769086982e-05\n",
      "418 8.130365398785399e-05\n",
      "419 7.795395538806402e-05\n",
      "420 7.474270277729943e-05\n",
      "421 7.16644621091365e-05\n",
      "422 6.871513288485298e-05\n",
      "423 6.588823915626221e-05\n",
      "424 6.317810458857619e-05\n",
      "425 6.0580715639194185e-05\n",
      "426 5.8091497308457744e-05\n",
      "427 5.570536425926863e-05\n",
      "428 5.341761790179621e-05\n",
      "429 5.122585356867753e-05\n",
      "430 4.912427105590473e-05\n",
      "431 4.710925007559185e-05\n",
      "432 4.517792454276021e-05\n",
      "433 4.332670843873268e-05\n",
      "434 4.1551972904090335e-05\n",
      "435 3.9850193982958995e-05\n",
      "436 3.821921012687855e-05\n",
      "437 3.665546709589055e-05\n",
      "438 3.515597386669109e-05\n",
      "439 3.3718803082299816e-05\n",
      "440 3.2341148385852346e-05\n",
      "441 3.1019783727932204e-05\n",
      "442 2.9752650598547654e-05\n",
      "443 2.8538135307057845e-05\n",
      "444 2.7373476553272843e-05\n",
      "445 2.6256718319367286e-05\n",
      "446 2.5186020022657793e-05\n",
      "447 2.415940827671802e-05\n",
      "448 2.317487580759617e-05\n",
      "449 2.2230640572820172e-05\n",
      "450 2.1325686075101068e-05\n",
      "451 2.0457550770029486e-05\n",
      "452 1.9625003901837315e-05\n",
      "453 1.8826624754658213e-05\n",
      "454 1.806108766099901e-05\n",
      "455 1.732684987065363e-05\n",
      "456 1.6622651682846143e-05\n",
      "457 1.5947676641703312e-05\n",
      "458 1.5300027005470755e-05\n",
      "459 1.4678942562719733e-05\n",
      "460 1.4083222988337763e-05\n",
      "461 1.3512079445292293e-05\n",
      "462 1.296400000445545e-05\n",
      "463 1.2438495166793617e-05\n",
      "464 1.1934434682591663e-05\n",
      "465 1.1450858449708648e-05\n",
      "466 1.0987035565439562e-05\n",
      "467 1.0542287481513541e-05\n",
      "468 1.0115586901068674e-05\n",
      "469 9.706235893041566e-06\n",
      "470 9.313687309438235e-06\n",
      "471 8.937095422704585e-06\n",
      "472 8.575854339501096e-06\n",
      "473 8.229275817302046e-06\n",
      "474 7.896885848207871e-06\n",
      "475 7.577934877267388e-06\n",
      "476 7.271961074834013e-06\n",
      "477 6.9785185423421165e-06\n",
      "478 6.6969302039736235e-06\n",
      "479 6.426782916859004e-06\n",
      "480 6.167649058802773e-06\n",
      "481 5.919030205975683e-06\n",
      "482 5.680492804544985e-06\n",
      "483 5.451661211457638e-06\n",
      "484 5.232104950259617e-06\n",
      "485 5.021441522926921e-06\n",
      "486 4.819299393729946e-06\n",
      "487 4.625440332636806e-06\n",
      "488 4.43934977071845e-06\n",
      "489 4.260807441853795e-06\n",
      "490 4.089525784021945e-06\n",
      "491 3.92514761948385e-06\n",
      "492 3.76742379188698e-06\n",
      "493 3.616107527042466e-06\n",
      "494 3.4708921406427535e-06\n",
      "495 3.3315229606609403e-06\n",
      "496 3.197812080427279e-06\n",
      "497 3.0695148147554964e-06\n",
      "498 2.9463746114931203e-06\n",
      "499 2.82822060103787e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    \n",
    "    # loss = (y_pred - y) ** 2\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    # \n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1210e-44, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3859, 0.6741, 0.8760],\n",
       "        [0.9284, 0.5427, 0.5447],\n",
       "        [0.2597, 0.3629, 0.2919],\n",
       "        [0.5440, 0.0099, 0.1426],\n",
       "        [0.2409, 0.9705, 0.7499]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3859, 0.9284, 0.2597, 0.5440, 0.2409],\n",
       "        [0.6741, 0.5427, 0.3629, 0.0099, 0.9705],\n",
       "        [0.8760, 0.5447, 0.2919, 0.1426, 0.7499]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.t_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3859, 0.9284, 0.2597, 0.5440, 0.2409],\n",
       "        [0.6741, 0.5427, 0.3629, 0.0099, 0.9705],\n",
       "        [0.8760, 0.5447, 0.2919, 0.1426, 0.7499]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3859, 0.6741, 0.8760],\n",
       "        [0.9284, 0.5427, 0.5447],\n",
       "        [0.2597, 0.3629, 0.2919],\n",
       "        [0.5440, 0.0099, 0.1426],\n",
       "        [0.2409, 0.9705, 0.7499]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3859, 0.6741, 0.8760],\n",
       "        [0.9284, 0.5427, 0.5447],\n",
       "        [0.2597, 0.3629, 0.2919],\n",
       "        [0.5440, 0.0099, 0.1426],\n",
       "        [0.2409, 0.9705, 0.7499]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3859, 0.9284, 0.2597, 0.5440, 0.2409],\n",
       "        [0.6741, 0.5427, 0.3629, 0.0099, 0.9705],\n",
       "        [0.8760, 0.5447, 0.2919, 0.1426, 0.7499]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.t>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3859, 0.9284, 0.2597, 0.5440, 0.2409],\n",
       "        [0.6741, 0.5427, 0.3629, 0.0099, 0.9705],\n",
       "        [0.8760, 0.5447, 0.2919, 0.1426, 0.7499]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.t>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3859, 0.9284, 0.2597, 0.5440, 0.2409],\n",
       "        [0.6741, 0.5427, 0.3629, 0.0099, 0.9705],\n",
       "        [0.8760, 0.5447, 0.2919, 0.1426, 0.7499]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3859, 0.6741, 0.8760],\n",
       "        [0.9284, 0.5427, 0.5447],\n",
       "        [0.2597, 0.3629, 0.2919],\n",
       "        [0.5440, 0.0099, 0.1426],\n",
       "        [0.2409, 0.9705, 0.7499]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9284, 0.5427, 0.5447])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3859, 0.9284, 0.2597, 0.5440, 0.2409],\n",
       "        [0.6741, 0.5427, 0.3629, 0.0099, 0.9705],\n",
       "        [0.8760, 0.5447, 0.2919, 0.1426, 0.7499]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.item>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'add_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-4c276cae5650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'add_'"
     ]
    }
   ],
   "source": [
    "a.add_(2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 4., 4., 4., 4.], dtype=torch.float64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.add_(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'matmul_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-23ba34db4cbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'matmul_'"
     ]
    }
   ],
   "source": [
    "b.matmul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.],\n",
       "        [3., 3.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x11fcb4f50>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(27., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-8b56c74c5abf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "x.grad\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x122d23950>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  4.,  7., 12.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.)\n",
    "x.requires_grad_(True)\n",
    "y = x* x+ 3\n",
    "z = y.sum()\n",
    "z\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  725.8818,   311.6635, -1158.4136], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1000])\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 100])\n",
      "torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "print(w1.shape)\n",
    "print(w2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 185.01754760742188\n",
      "199 0.21314485371112823\n",
      "299 0.0006823792355135083\n",
      "399 4.681038626586087e-05\n",
      "499 1.5293464457499795e-05\n"
     ]
    }
   ],
   "source": [
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9384, -0.3983, -0.2297], requires_grad=True)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.8769, -0.7965, -0.4594], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0900)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.data.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (untitled8)",
   "language": "python",
   "name": "pycharm-25c6eaff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
